import torch
from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration

# Use a more advanced conversational model
model_name = "facebook/blenderbot-400M-distill"
tokenizer = BlenderbotTokenizer.from_pretrained(model_name)
model = BlenderbotForConditionalGeneration.from_pretrained(model_name)

# Function to generate response
def generate_response(user_input, conversation_history, max_length=100):
    full_input = f"{conversation_history}Human: {user_input}"
    inputs = tokenizer([full_input], return_tensors="pt", padding=True, truncation=True, max_length=512)
    
    reply_ids = model.generate(
        **inputs,
        max_length=max_length,
        num_return_sequences=1,
        no_repeat_ngram_size=3,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7
    )
    
    response = tokenizer.decode(reply_ids[0], skip_special_tokens=True)
    return response

# Main chatbot loop with conversation management
def chat_loop():
    conversation_history = ""
    print("AI: Hello! I'm an AI assistant. How can I help you today?")
    while True:
        user_input = input("You: ")
        if user_input.lower() in ['quit', 'exit', 'bye']:
            print("AI: It was nice talking to you. Goodbye!")
            break
        elif user_input.lower() == 'clear history':
            conversation_history = ""
            print("AI: Conversation history cleared. What would you like to talk about?")
            continue
        
        response = generate_response(user_input, conversation_history)
        print("AI:", response)
        
        # Updating conversation history
        conversation_history += f"Human: {user_input}\nAI: {response}\n"
        conversation_history = keep_recent_conversation(conversation_history)

# Function to keep only recent conversation
def keep_recent_conversation(history, max_tokens=1000):
    tokens = tokenizer.encode(history)
    if len(tokens) > max_tokens:
        decoded = tokenizer.decode(tokens[-max_tokens:])
        return decoded[decoded.find("\n")+1:]  # Start from the first complete line
    return history

# Run the chatbot
if __name__ == "__main__":
    chat_loop()
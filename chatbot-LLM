import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import numpy as np

# Use a more advanced model
model_name = "facebook/opt-1.3b"  # Upgraded to a larger model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Add this line before using the tokenizer
tokenizer.pad_token = tokenizer.eos_token

# Function to generate response with dynamic length
def generate_response(user_input, conversation_history, max_length=200):
    full_input = f"{conversation_history}Human: {user_input}\nAI:"
    inputs = tokenizer(full_input, return_tensors="pt", padding=True, truncation=True)
    
    # Dynamic max_length based on input length
    dynamic_max_length = min(max(len(inputs.input_ids[0]) + 50, max_length), 512)
    
    output = model.generate(
        inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_length=dynamic_max_length,
        num_return_sequences=3,  # Generate multiple responses
        no_repeat_ngram_size=3,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.7,
        pad_token_id=tokenizer.pad_token_id
    )
    
    # Select the best response based on perplexity
    responses = [tokenizer.decode(seq, skip_special_tokens=True).split("AI:")[-1].strip() for seq in output]
    perplexities = [calculate_perplexity(response) for response in responses]
    best_response = responses[np.argmin(perplexities)]
    
    return best_response

# Function to calculate perplexity
def calculate_perplexity(text):
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
    return torch.exp(outputs.loss).item()

# Main chatbot loop with conversation management
def chat_loop():
    conversation_history = ""
    print("AI: Hello! How can I help you today?")
    while True:
        user_input = input("You: ")
        if user_input.lower() == 'quit':
            print("AI: Goodbye!")
            break
        elif user_input.lower() == 'clear history':
            conversation_history = ""
            print("AI: Conversation history cleared.")
            continue
        
        response = generate_response(user_input, conversation_history)
        print("AI:", response)
        
        # Updating conversation history with a sliding window
        conversation_history += f"Human: {user_input}\nAI: {response}\n"
        conversation_history = keep_recent_conversation(conversation_history)

# Function to keep only recent conversation
def keep_recent_conversation(history, max_tokens=1000):
    tokens = tokenizer.encode(history)
    if len(tokens) > max_tokens:
        decoded = tokenizer.decode(tokens[-max_tokens:])
        return decoded[decoded.find("\n")+1:]  # Start from the first complete line
    return history

# Run the chatbot
if __name__ == "__main__":
    chat_loop()